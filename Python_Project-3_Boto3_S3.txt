Boto-3 Projects:

Boto-3 is a python package that is used to interact with AWS.

Task:

To create an ec2 instance or s3 bucket on AWS programmatically using python, then the module that you will use to interact with AWS API's is Boto3 module.

What is Boto-3 in a nutshell ?

- If you are asked to write a python script, and using this python script you are asked to create resources on AWS. Probably you want to create an S3 Bucket, there is an API for S3 bucket.
- So using python, if you want to talk to this API and create resources in AWS and get the result back displayed to the user.
- The module that you will use to do all of these things in python is Boto 3.

What about requests module ?

- We can use requests module, but the problem is that there will be hundred's of API's and there will be issues in authentication and many obstacles.
- This abstracting will be taken care by Boto3.
- Using requests module, if you are able to do in 100 lines. With boto3 module, it can be done in 20-30 lines.

Note:
Basically, Boto3 abstract a lot of things that you don't need to learn serious programming.


Pre-requisites:
- Need to learn how to use AWS from UI, because you need to know what are the mandatory fields that need to be included and descriptions of each and every thing.
- Basically Boto3 is an automation ( A person who knows manual steps can perform automation in a better way. 


Important points:

- AWS CLI, Terraform, Cloud Formation Templates, Boto 3 all are used to perform same thing, all of them are used to create and manage resources on AWS.

Then why to learn Boto3 ?

- There will be 2 categories, one is AWS CLI and Boto3 ; other is CFT and Terraform
- CFT and Terraform come under templating languages because you don't need to learn programming or something but you will have some template files (Heat templates or cloud formation 
templates )
- If you know how to create these templates, you have created the infrastructure.
- Where as AWS CLI and Boto3 are scripting languages, if you have to create more than one resource or run more than one command in AWS CLI you use bash scripting ; if you have to do 
same thing using Boto3 then you have to use python scripting.
- The main difference between both categories is you will scripting for AWS CLI and Boto3, templating for CFT and Terraform.


A question may arise ?

- If we are able to do things with templating, then why we need to learn scripting ?
- Let's say you want to create an ec2 instance or list out all the s3 buckets, using AWS CLI you will just use "aws s3 ls", but with Terraform you need to install, set up all the files and write templates which is not a quick action.
- If you need to get the information quickly then scripting need to be your choice instead of templating. Here scripting or programming will be easy for quick outputs.

Again why Boto3 when we have AWS CLI ?

- The main advantage is that Boto3 can be used in the serverless programming.
- That means, in AWS you know that there are Lambda functions where using lambda functions devops engineers perform some projects ( cloud cost optimization, resource monitoring, sending 
notifications if any alerts triggered )
- To do all above things, you will use lambda functions which will use serverless programming and for that there is no option to use shell scripting, you acn only use python, nodejs, golang programs.


Practical:

Use vscode or github workspaces; if github workspaces, download the AWS cli dev container and configure the AWS.

Steps:
1) After configuring, open a new file x.py and install the boto3 module in the terminal (pip install boto3); if an error occurs, you can also try (pip3 install boto3).
2) Then first import Boto3.

Task: To create a S3 bucket

1) Go to the Boto3 documentation.
2) Go to Boto3 AWS; after going there, scroll down and search for available services. Following search for S3.
3) First, create a client to connect with AWS:
   
   import boto3

client = boto3.client('s3')

4) Next, search for whatever task you want to perform, for example, creating a S3 bucket (which will be present in the available methods).
5) Here will be the "create bucket" available method; after that, scroll down, and there will be a request syntax and other codes on how to perform. Copy and paste the request syntax.
Note: There will be many fields in the syntax; all won't be required. That's why creating through UI, or manual creation, is very important. Otherwise, we will not know what is required or necessary.

6) Keep the required fields
Note: With the help of the Boto3 documentation, you can know which fields are required. There will be a section on parameters, where it will list the required attributes. By referring to that,
We can keep whatever parameters are required.

7) For S3, a bucket parameter is required. For EC2, an ami ID and instance type are required.
Note: Here, for S3, the bucket parameter means naming the string. Try to keep the names as unique as possible.
   
Here are some common requirements for bucket names:

Bucket names must be globally unique across all existing bucket names in AWS.
Bucket names must be between 3 and 63 characters in length.
Bucket names can only contain lowercase letters, numbers, hyphens (-), and dots (.).
Bucket names must start and end with a lowercase letter or number.
   Bucket names cannot be formatted as IP addresses (e.g., "192.168.0.1").

8) In this way, you talk to S3. You have 2 ways to create S3.

way1->client = boto3.client("s3") - going ahead, client needs to be used, supports botocore, which handles exceptions 
way2->resource = boto3.resource("s3") - highly simple, very abstract, not available

Botocore has all kinds of exceptions, which you don't have to define.

Tasks: Explore how to create resources using S3.

   

Challenges Faced:
1) Configuring AWS and rebuilding issues in Github codespaces
2) Searching Boto3 documentation
3) I got to learn many things while implementing and debugging, e.g., how the names of the S3 bucket should be, file paths, and so on.

Chatgpt:

Certainly, here's a more technical alignment of the steps:

Practical Steps:
Setting up Environment:
Configuration: Configure AWS CLI in the development environment (VSCode or GitHub Workspaces).
Implementation:
File Creation and Module Installation:

Open a new Python file, e.g., x.py.
Install the boto3 module using Terminal: pip install boto3 or pip3 install boto3 if necessary.
Import Boto3:

python
Copy code
import boto3
Task: Creating an S3 Bucket
Documentation Exploration:

Navigate to the Boto3 documentation for AWS services.
Locate S3 Service:

Find the S3 service among the available AWS services.
Client Initialization:

Create a client to interact with S3:
python
Copy code
client = boto3.client('s3')
Method Identification:

Search for the desired task, e.g., creating an S3 bucket, among the available methods.
Request Syntax:

Copy the request syntax for the create bucket method from the documentation.
Parameter Selection:

Identify and select the required parameters for the bucket creation.
Refer to the documentation's parameter section to determine mandatory attributes.
Bucket Naming Considerations:

Follow the naming conventions for S3 buckets:
Globally unique names.
Length between 3 and 63 characters.
Lowercase letters, numbers, hyphens, and dots only.
Must start and end with a lowercase letter or number.
Cannot be formatted as IP addresses.
Implementation Approaches:

There are two approaches to interact with S3:
Using client: Direct interaction, supports botocore with built-in exception handling.
Using resource: Provides higher-level abstraction, simplifies interaction, but may lack certain functionalities.

Technical Notes:
Client vs. Resource:
client = boto3.client("s3"): Direct interaction with S3, supports botocore for handling exceptions.
resource = boto3.resource("s3"): Abstracted approach, simpler but may lack some functionalities.
Exception Handling:
Botocore handles exceptions automatically, reducing the need for explicit error handling in the code.

Resume Point:

Technical Skills
Cloud Computing:
Experience with AWS services including S3.
Implemented S3 bucket creation using Boto3 Python library.
Familiarity with AWS CLI and AWS SDK.
Experience
Project XYZ (Date Range)
Contributed to the implementation of cloud infrastructure on AWS.
Created S3 buckets programmatically using Boto3 library in Python.
Collaborated with team members to ensure adherence to AWS best practices and security guidelines.
In your resume, it's important not just to list the tasks you've completed but also to highlight your contributions and the context in which you've applied your skills. This helps potential employers understand the depth of your experience and how it aligns with their needs.

Certainly, here's a concise version:

Created S3 buckets programmatically using Boto3 Python library, demonstrating proficiency in cloud infrastructure setup and automation.
Implemented AWS S3 bucket creation utilizing Boto3 library, showcasing hands-on experience in cloud computing and infrastructure as code (IaC).
Developed S3 bucket creation automation using Boto3 Python library, showcasing practical skills in AWS cloud services and infrastructure automation.



In my previous role, I successfully demonstrated my proficiency in cloud infrastructure setup and automation by creating S3 buckets programmatically using the Boto3 Python library. This experience highlighted my practical skills in cloud computing and infrastructure as code (IaC), showcasing an ability to implement AWS services efficiently and effectively. By developing S3 bucket creation automation, I contributed to streamlining processes and enhancing scalability within our cloud environment, underscoring my commitment to leveraging technology for optimal results.

